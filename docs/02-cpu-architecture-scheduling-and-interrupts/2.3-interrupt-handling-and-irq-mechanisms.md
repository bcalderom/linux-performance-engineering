# 2.3 Interrupt Handling & IRQ Mechanisms

This document explains how hardware devices get the CPU’s attention, how the kernel processes these signals, and why "system time" can spike even when your applications are idle.

**Scope:**
- The difference between Hardware Interrupts (Top Half) and Softirqs (Bottom Half).
- How to analyze `/proc/interrupts` to find hardware hotspots.
- Understanding `ksoftirqd` and `%si` CPU usage.
- Recognizing interrupt storms and single-core bottlenecks.

**Target Audience:**
Sysadmins and SREs who need to diagnose high system load (`%sys` or `%si`) or debug network throughput issues that seem to hit a "ceiling" despite available CPU capacity.

---

## 2.3.1 Key Terms

- **Hardware Interrupt (IRQ)**
  An electrical signal from a device (NIC, disk controller, timer) that forces the CPU to stop executing the current instruction stream and transfer control to a kernel handler.
  **Observable:** `vmstat 1` (column `in`), `/proc/interrupts`.

- **Interrupt Service Routine (ISR)**
  The kernel function that executes in response to a hardware interrupt. ISRs run in **interrupt context**, not process context, and must complete quickly without sleeping.
  **Observable:** Indirectly via `%hi` in `top`; ISR execution time contributes to this metric.

- **Interrupt Context**
  A special CPU **execution mode** where no process is "current." Code running in interrupt context cannot sleep, allocate memory with `GFP_KERNEL`, or access user-space memory. It uses the interrupted task's kernel stack (or a dedicated IRQ stack).
  **Observable:** Kernel code paths; not directly visible, but violations cause kernel panics.

- **Interrupt Vector**
  A numeric index (0–255 on x86) into the Interrupt Descriptor Table (IDT) that maps an interrupt source to its handler function. IRQ numbers are translated to vectors by the interrupt controller.
  **Observable:** `/proc/interrupts` shows IRQ numbers; vectors are internal.

- **Hard IRQ (Top Half)**
  The ISR code that runs immediately when a hardware interrupt arrives. It acknowledges the device, captures minimal state, and schedules deferred work. Runs with local interrupts disabled.
  **Observable:** `top` (column `%hi`).

- **SoftIRQ (Bottom Half)**
  Deferred interrupt processing that runs after the Hard IRQ completes. SoftIRQs run with interrupts **enabled** and handle heavy work like TCP/IP processing. There are 10 kernel-defined softirq types.
  **Observable:** `top` (column `%si`), `/proc/softirqs`, process `ksoftirqd/N`.

- **Interrupt Affinity**
  The configuration determining which CPU cores are allowed to handle interrupts from specific devices.
  **Observable:** `/proc/irq/<IRQ>/smp_affinity`, `/sys/kernel/irq/<N>/`.

- **MSI/MSI-X (Message Signaled Interrupts)**
  Modern interrupt delivery mechanism where devices write to a special memory address instead of using dedicated IRQ lines. MSI-X supports multiple interrupt vectors per device, enabling multi-queue NICs and NVMe drives.
  **Observable:** `lspci -v | grep MSI`, `/proc/interrupts` (devices with `-TxRx-` suffixes).

---

## 2.3.2 What Is an Interrupt?

**Interrupt Processing Flow (Overview)**

```
  [Device]  --IRQ Signal-->  [APIC]  --Vector-->  [IDT]
                                                    |
                                                    v
                                    +-------------------------------+
                                    |    HARD IRQ / ISR (Top Half)  |
                                    +-------------------------------+
                                                    |
                                                    v
                                    +-------------------------------+
                                    |    SOFTIRQ (Bottom Half)      |
                                    +-------------------------------+
                                                    |
                                                    v
                                    +-------------------------------+
                                    |    SCHEDULER CHECK            |
                                    +-------------------------------+
                                                    |
                                                    v
                                            [Resume Task]
```

| Stage | What Happens | Interrupts | Duration |
|-------|--------------|------------|----------|
| **1. Hard IRQ (Top Half)** | Acknowledge device, save state, raise softirq | DISABLED | Microseconds |
| **2. SoftIRQ (Bottom Half)** | Process packets, complete I/O, run timers | ENABLED | μs to ms |
| **3. Scheduler Check** | Decide: resume interrupted task or switch to another | — | — |

If the CPU had to ask every device "Do you have data?" (Polling) millions of times a second, it would get nothing else done. Instead, devices "interrupt" the CPU.

**1. Plain-language explanation**
An interrupt is a hardware notification system. It forces the CPU to pause the current program, run a specific handler function, and then resume the program exactly where it left off.

**2. Simple analogy**
- **Polling:** You constantly check your mailbox every 10 seconds to see if mail arrived. (Wastes your time).
- **Interrupt:** The mail carrier rings your doorbell. You stop what you're doing, get the mail, and go back to work.

**3. Technical detail**
When an electrical signal arrives on an IRQ line, the CPU saves the execution state (registers) of the current task and jumps to a handler function defined in the kernel's Interrupt Descriptor Table (IDT).

> **Terminology Note: ISR vs. Hard IRQ**
> 
> These terms refer to the **same thing**—the top-half handler code that runs immediately when a hardware interrupt arrives:
> - **ISR (Interrupt Service Routine):** The traditional, hardware-centric term from computer architecture.
> - **Hard IRQ (Top Half):** The Linux kernel term, emphasizing the contrast with "Soft IRQ" (bottom half).
> 
> When you read "ISR" or "Hard IRQ handler," think: *the tiny, urgent code that runs with interrupts disabled to acknowledge the device and schedule deferred work.*

**4. Observable behavior**
Run `vmstat 1`. The `in` column shows the number of **Interrupts per Second**.

```bash
vmstat 1
```
```text
procs ... --system-- ...
 r  b ...  in   cs ...
 1  0 ... 250  600 ...
```

**Why This Matters:**
A sudden spike in `in` (e.g., from 200 to 20,000) means hardware is screaming for attention. If the CPU spends all its time answering the doorbell, it can't do any actual work inside the house.

---

### 2.3.2.1 ISR, Interrupt Context, and the IDT

To understand what happens when an interrupt fires, we need to examine three tightly-coupled concepts: the **ISR**, **interrupt context**, and the **IDT**.

**What is an ISR (Interrupt Service Routine)?**

An ISR is the kernel function that handles a specific interrupt. When device X triggers IRQ 24, the kernel runs the ISR registered for IRQ 24.

**1. Plain-language explanation**
The ISR is the "emergency responder" for a specific device. It runs immediately when that device signals, does the absolute minimum work, and gets out.

**2. Simple analogy**
Imagine a fire alarm. The ISR is the person who:
1. Acknowledges the alarm (stops it from ringing)
2. Notes which room triggered it
3. Calls the fire department (schedules the softirq)
4. Returns to their desk

They do NOT fight the fire themselves—that's the softirq's job.

**3. Technical detail**
When an interrupt arrives:
1. The CPU saves the current instruction pointer and flags to the stack
2. The CPU looks up the **interrupt vector** in the **Interrupt Descriptor Table (IDT)**
3. The IDT entry points to the ISR function address
4. The CPU jumps to the ISR with interrupts disabled on this core
5. The ISR runs in **interrupt context** (not process context)
6. The ISR returns, the CPU restores state, and execution resumes

**The Interrupt Descriptor Table (IDT)**

The IDT is a 256-entry table in memory that maps interrupt vectors (0–255) to handler functions:

| Vector Range | Purpose |
|--------------|---------|  
| 0–31 | CPU exceptions (divide-by-zero, page fault, etc.) |
| 32–47 | Legacy IRQs (remapped by PIC/APIC) |
| 48–255 | Available for device interrupts (MSI/MSI-X) |

The APIC (Advanced Programmable Interrupt Controller) translates device IRQ numbers into vectors and routes them to specific CPUs.

**What is Interrupt Context?**

Interrupt context is a special execution environment with strict rules:

| Process Context | Interrupt Context |
|-----------------|-------------------|
| Has a "current" process | No current process |
| Can sleep/block | **Cannot sleep** |
| Can allocate memory freely | Limited allocation (GFP_ATOMIC only) |
| Can access user-space | **Cannot access user-space** |
| Uses process's kernel stack | Uses IRQ stack or interrupted task's stack |
| Preemptible (usually) | **Not preemptible** |

**Why ISRs Cannot Sleep**

Sleeping requires a process context to "wake up" later. In interrupt context, there's no process to suspend—the CPU is servicing hardware. If an ISR tried to sleep:
1. The scheduler would have no task to switch away from
2. The interrupt would never complete
3. The system would deadlock or panic

**Why This Matters:**
Understanding interrupt context explains why kernel drivers are complex: any code that might run in an ISR must be carefully written to never sleep, never take sleeping locks, and never access user memory.

**4. Observable behavior**
You cannot directly observe ISR execution, but you can infer it:

```bash
# See which IRQs are active and their handlers
cat /proc/interrupts

# See IRQ-to-device mapping
ls -la /sys/kernel/irq/*/actions
```

**Cause–Effect Table:**

| Cause | Effect | Why |
|-------|--------|-----|
| Device signals IRQ | CPU stops current task | Hardware forces immediate attention |
| CPU looks up IDT | Finds ISR address | Vector → handler mapping |
| ISR runs | Interrupts disabled locally | Prevent nested interrupt chaos |
| ISR raises softirq | Deferred work scheduled | Heavy work can't run in ISR |
| ISR returns | Softirq runs (maybe) | If pending and allowed |

---

### 2.3.2.2 Interrupts vs. Context Switches: A Critical Distinction

A common misconception: **interrupts do NOT automatically cause context switches**.

**1. Plain-language explanation**
An interrupt is a *mode switch* (user→kernel or kernel→interrupt), not a *task switch*. The CPU temporarily handles the interrupt and then resumes exactly where it left off—same task, same memory, same everything.

**2. Simple analogy**
You're reading a book. The phone rings (interrupt). You answer it, take a message, hang up. You go back to reading the same book, same page, same paragraph. You didn't switch to a different book.

A context switch would be: you answer the phone, and someone else sits down and starts reading a *different* book.

**3. Technical detail**
Interrupts may *trigger* a context switch, but only indirectly:

| Scenario | Does Context Switch Happen? |
|----------|-----------------------------|
| Simple interrupt, no wake-ups | **No** – resume interrupted task |
| Interrupt wakes higher-priority task | **Yes** – scheduler runs on IRQ return |
| Timer interrupt expires timeslice | **Yes** – scheduler preempts current task |
| Network packet completes blocked I/O | **Yes** – waiting task becomes runnable |

**The key insight:** The scheduler only runs at specific points:
- System call return
- **Interrupt return** (if `need_resched` flag is set)
- Explicit `schedule()` calls

The ISR itself never calls the scheduler. Instead, it sets `need_resched` if it wakes a task, and the scheduler runs *after* the ISR completes.

**Concept Chain:**
> **Interrupt fires** → **ISR runs** → **ISR wakes task** → **ISR sets need_resched** → **ISR returns** → **Scheduler checks flag** → **Context switch (maybe)**

**Why This Matters:**
This explains why high interrupt rates don't directly correlate with high context switch rates. A server handling 100,000 interrupts/sec might have only 1,000 context switches/sec if most interrupts don't wake new tasks.

**4. Observable behavior**
```bash
# Compare interrupt rate vs context switch rate
vmstat 1
```
```text
procs ... --system-- ...
 r  b ...  in    cs ...
 1  0 ... 50000  1200 ...   # 50k interrupts, only 1.2k context switches
```

---

## 2.3.3 The Two-Halves Model: Hard vs. Soft IRQs

**Terminology Clarification:**
- **Hardware Interrupt** = the electrical signal from the device
- **Hard IRQ** = the ISR (top-half handler) that responds to the hardware interrupt

These terms are often used interchangeably, but technically: the hardware interrupt is the *event*, and the Hard IRQ handler is the *code that responds*.

Interrupt processing is expensive. To minimize disruption, Linux splits the work.

**1. Plain-language explanation**
The "Hard IRQ" is the quick acknowledgment ("I heard you, stop ringing!"). The "SoftIRQ" is the actual work ("Okay, let me read this letter and file it").

**2. Simple analogy**
- **Hard IRQ (Receptionist):** Answers the phone immediately, takes a message, hangs up. (Fast, urgent).
- **SoftIRQ (Caseworker):** Picks up the message later and processes the request. (Slower, can wait).

**Concept Chain:**
> **Packet Arrives** → **CPU Interrupted** → **Hard IRQ (Ack Device)** → **SoftIRQ (Process TCP/IP)** → **User Application**

**3. Technical detail**
- **Hard IRQ:** Runs with local interrupts disabled. Crucial to finish fast to avoid losing other interrupts.
- **SoftIRQ:** Runs with interrupts enabled. Can be preempted. Handles heavy lifting like protocol stack processing or block I/O.

**Why This Matters:**
This split explains why you see two different CPU metrics in `top`: `%hi` (Hardware IRQ) and `%si` (Software IRQ). High `%hi` is rare and dangerous (hardware broken? driver bug?). High `%si` is common (heavy network/disk load).

---

## 2.3.4 Observing Interrupt Sources

To see *who* is ringing the doorbell, check `/proc/interrupts`.

```bash
head -n 20 /proc/interrupts
```

```text
           CPU0       CPU1       CPU2       CPU3
 24:    1234567          0          0          0   IO-APIC  24-fasteoi  eth0
 25:         10      23456          0          0   IO-APIC  25-fasteoi  ahci
```

**Interpretation:**
- **Columns:** The number of interrupts handled by each CPU core.
- **Rightmost Column:** The device name (`eth0`, `ahci`, `nvidia`).
- **Imbalance:** In the example above, `eth0` (Network) is **only** interrupting CPU0. CPU1, 2, and 3 are doing nothing for `eth0`.

> **Micro-Summary:**
> We know that devices trigger **Interrupts**, which split into **Hard** and **Soft** parts. We can identify the specific device using `/proc/interrupts`. Next, we look at what happens when there are *too many* interrupts.

---

### 2.3.4.1 MSI, MSI-X, and Modern Interrupt Delivery

Legacy interrupt delivery used dedicated IRQ lines (wires) from devices to the interrupt controller. Modern systems use **Message Signaled Interrupts (MSI/MSI-X)**, which are fundamentally different.

**1. Plain-language explanation**
Instead of pulling a dedicated wire, the device writes a small message to a special memory address. The CPU sees this write and treats it as an interrupt.

**2. Simple analogy**
- **Legacy IRQ:** Each device has its own doorbell wire running to your house.
- **MSI:** Devices send text messages to your phone. No wires needed, and each device can send different message types.

**3. Technical detail**

| Feature | Legacy (IO-APIC) | MSI | MSI-X |
|---------|------------------|-----|-------|
| Delivery | Dedicated IRQ lines | Memory write | Memory write |
| Vectors per device | 1 | 1–32 | Up to 2048 |
| Sharing | Often shared | Not shared | Not shared |
| Multi-queue support | No | Limited | **Yes** |
| Latency | Higher | Lower | Lowest |

**Why MSI-X Matters for Performance:**

Modern NICs and NVMe drives use **multi-queue** architectures:
- A 10GbE NIC might have 8 RX queues and 8 TX queues
- Each queue gets its own MSI-X interrupt vector
- Each vector can be pinned to a different CPU core
- Result: **True parallel packet processing**

**Concept Chain:**
> **Multi-queue NIC** → **MSI-X vectors** → **Per-CPU interrupt affinity** → **Parallel softirq processing** → **Linear scaling**

**4. Observable behavior**

```bash
# Check if device uses MSI/MSI-X
lspci -v | grep -i msi
```
```text
        Capabilities: [50] MSI-X: Enable+ Count=64 Masked-
```

```bash
# See multi-queue NIC interrupts (note the -TxRx- naming)
cat /proc/interrupts | grep eth
```
```text
 45:    123456  0       0       0   PCI-MSI  524289-edge  eth0-TxRx-0
 46:    0       234567  0       0   PCI-MSI  524290-edge  eth0-TxRx-1
 47:    0       0       345678  0   PCI-MSI  524291-edge  eth0-TxRx-2
 48:    0       0       0       456789  PCI-MSI  524292-edge  eth0-TxRx-3
```

Notice each queue (`TxRx-0`, `TxRx-1`, etc.) has its own IRQ line and is handled by a different CPU.

**Inspecting IRQ-to-Device Mapping:**

```bash
# Find device associated with IRQ 45
cat /sys/kernel/irq/45/actions
ls -la /sys/kernel/irq/45/

# See all IRQs for a device
ls /sys/class/net/eth0/device/msi_irqs/
```

**Why This Matters:**
If you see all NIC interrupts going to one CPU despite having a multi-queue card, check:
1. Is MSI-X enabled? (`lspci -v`)
2. Is `irqbalance` running?
3. Are the queues configured? (`ethtool -l eth0`)

---

## 2.3.5 Softirqs and ksoftirqd

When SoftIRQ work gets too heavy (e.g., 10Gbps network traffic), the kernel offloads it to special threads to protect user tasks from being totally starved.

**Observable:** `ksoftirqd/N`
These are per-CPU kernel threads (e.g., `ksoftirqd/0` for CPU 0).

| Metric | Meaning | Good/Bad |
|--------|---------|----------|
| `%hi` | Time in Hard IRQ | **>1% is Bad.** Usually indicates a hardware storm or broken driver. |
| `%si` | Time in SoftIRQ | **>30% is Concern.** High network/disk throughput. |
| `ksoftirqd` | Kernel Thread | If this process uses 100% of a core, that core is saturated with network/disk work. |

**Why This Matters:**
If `ksoftirqd/0` is at 100% usage, your application running on CPU 0 will slow down or stall, even if CPU 1-3 are idle. This is a classic **Single-Core Bottleneck**.

---

### 2.3.5.1 Softirq Types and /proc/softirqs

The kernel defines exactly **10 softirq types**, each with a numeric ID and specific purpose:

| ID | Name | Purpose | Common Trigger |
|----|------|---------|----------------|
| 0 | `HI_SOFTIRQ` | High-priority tasklets | Rarely used |
| 1 | `TIMER_SOFTIRQ` | Timer callbacks | `hrtimer`, `timer_list` expiry |
| 2 | `NET_TX_SOFTIRQ` | Network transmit | Packet transmission completion |
| 3 | `NET_RX_SOFTIRQ` | Network receive | **Packet arrival (NAPI)** |
| 4 | `BLOCK_SOFTIRQ` | Block device completion | Disk I/O completion |
| 5 | `IRQ_POLL_SOFTIRQ` | IRQ polling | Block device polling |
| 6 | `TASKLET_SOFTIRQ` | Tasklet execution | Driver-scheduled tasklets |
| 7 | `SCHED_SOFTIRQ` | Scheduler load balancing | Per-CPU load balancing |
| 8 | `HRTIMER_SOFTIRQ` | High-res timer (legacy) | Deprecated, rarely seen |
| 9 | `RCU_SOFTIRQ` | RCU callbacks | Read-Copy-Update cleanup |

**Observable:** `/proc/softirqs`

```bash
cat /proc/softirqs
```
```text
                    CPU0       CPU1       CPU2       CPU3
          HI:          0          0          0          0
       TIMER:    1234567     987654     876543     765432
      NET_TX:      12345       9876       8765       7654
      NET_RX:    9876543    8765432    7654321    6543210
       BLOCK:     543210     432109     321098     210987
    IRQ_POLL:          0          0          0          0
     TASKLET:       1234        987        876        765
       SCHED:     654321     543210     432109     321098
     HRTIMER:          0          0          0          0
         RCU:    2345678    1234567     987654     876543
```

**Interpretation:**
- **NET_RX** dominates on network-heavy servers
- **BLOCK** dominates on storage-heavy servers
- **TIMER** and **SCHED** are always present (kernel housekeeping)
- **RCU** increases with kernel activity (memory reclamation)

**Why This Matters:**
When diagnosing high `%si`, check `/proc/softirqs` to identify *which* softirq type is responsible. High `NET_RX` points to network; high `BLOCK` points to disk.

---

### 2.3.5.2 When Do Softirqs Run?

Softirqs don't run inside the ISR. They run at specific points:

1. **Immediately after ISR returns** (if softirqs are pending and not rate-limited)
2. **On return from interrupt to user-space**
3. **In `ksoftirqd` kernel thread** (if too much work accumulated)

**The Rate-Limiting Mechanism:**

To prevent softirqs from starving user tasks, the kernel limits how long softirqs can run:
- After processing, if more softirqs are pending AND
- The softirq has run for "too long" (configurable)
- Then: wake `ksoftirqd` to continue later

**Cause–Effect Table:**

| Situation | Softirq Runs Where? | Why? |
|-----------|---------------------|------|
| Light load, few packets | Inline after ISR | Fast, no thread switch needed |
| Moderate load | Inline, then ksoftirqd | Fairness to user tasks |
| Heavy load (10Gbps) | Mostly ksoftirqd | Prevent user starvation |
| Interrupt disabled too long | ksoftirqd only | Softirqs can't run inline |

**Key Insight:** Softirqs run with **interrupts enabled**. This means a softirq processing packets can itself be interrupted by a new hardware interrupt. This is different from ISRs, which run with interrupts disabled.

---

### 2.3.5.3 NAPI: Interrupt Mitigation for Networking

Modern NICs don't generate one interrupt per packet. That would cause **interrupt storms** at 10Gbps+ speeds (millions of packets/sec = millions of interrupts/sec).

**NAPI (New API)** is the kernel's solution: a hybrid interrupt/polling model.

**1. Plain-language explanation**
Instead of ringing the doorbell for every letter, the mail carrier rings once, then you check the mailbox repeatedly until it's empty.

**2. Simple analogy**
- **Without NAPI:** Doorbell rings 1000 times for 1000 letters. You answer 1000 times.
- **With NAPI:** Doorbell rings once. You grab all 1000 letters in one trip. Doorbell re-enabled when mailbox empty.

**3. Technical detail**

**NAPI Flow:**
```
1. Packet arrives → Hardware interrupt fires
2. ISR runs → Disables further interrupts from this queue
3. ISR schedules NAPI poll → Raises NET_RX_SOFTIRQ
4. Softirq runs napi_poll() → Processes up to "budget" packets (default: 64)
5. If more packets: re-run poll
6. If queue empty: re-enable interrupts
```

**Concept Chain:**
> **Packet flood** → **Single interrupt** → **NAPI poll loop** → **Process 64 packets** → **Re-poll or re-arm** → **Reduced interrupt overhead**

**Why This Matters:**
NAPI explains a counter-intuitive observation: under heavy network load, `/proc/interrupts` counts may **decrease** while throughput increases. The NIC is spending more time in polling mode, fewer interrupts needed.

**4. Observable behavior**

```bash
# See NAPI budget (packets per poll)
sysctl net.core.netdev_budget
```
```text
net.core.netdev_budget = 300
```

```bash
# Watch interrupt rate vs packet rate
watch -n 1 'cat /proc/interrupts | grep eth; echo "---"; cat /proc/net/dev | grep eth'
```

Under heavy load, you'll see packet counts rising much faster than interrupt counts.

---

## 2.3.6 Interrupt Storms, Throttling, and Mitigation

An **interrupt storm** occurs when a device generates interrupts faster than the system can process them, consuming all CPU time in interrupt handling.

**1. Plain-language explanation**
The doorbell is ringing so fast that you spend all day answering it and never get any work done.

**2. Causes of Interrupt Storms**

| Cause | Symptom | Diagnosis |
|-------|---------|----------|
| Broken NIC/driver | `%hi` > 50% | `dmesg` errors, `ethtool -S` |
| Flood of small packets | `%si` = 100% on one core | `/proc/interrupts` imbalance |
| Misconfigured interrupt coalescing | High interrupt rate, low throughput | `ethtool -c` |
| Shared IRQ conflict | Multiple devices on same IRQ | `/proc/interrupts` |
| DDoS attack | Sudden spike in `NET_RX` softirqs | `/proc/softirqs`, `iftop` |

**3. Interrupt Coalescing (Adaptive Moderation)**

NICs can delay interrupts to batch multiple events:

```bash
# View current coalescing settings
ethtool -c eth0
```
```text
Coalesce parameters for eth0:
Adaptive RX: on  TX: on
rx-usecs: 50
rx-frames: 64
tx-usecs: 50
tx-frames: 64
```

- **rx-usecs:** Wait up to N microseconds before interrupting
- **rx-frames:** Or interrupt after N frames, whichever comes first
- **Adaptive:** NIC auto-tunes based on traffic patterns

**Tuning Trade-off:**

| Setting | Latency | Throughput | CPU Usage |
|---------|---------|------------|----------|
| Low coalescing | Lower | Lower | Higher |
| High coalescing | Higher | Higher | Lower |
| Adaptive | Balanced | Balanced | Balanced |

**4. When irqbalance Helps vs. Hurts**

`irqbalance` is a daemon that distributes interrupts across CPUs.

**When it helps:**
- Generic servers with mixed workloads
- Systems without careful IRQ tuning
- Multi-queue NICs that need spreading

**When it hurts:**
- Latency-sensitive applications (interrupts may move away from app's CPU)
- Carefully tuned NUMA systems
- Real-time workloads

```bash
# Check if irqbalance is running
systemctl status irqbalance

# See current IRQ affinity
for i in /proc/irq/*/smp_affinity; do echo "$i: $(cat $i)"; done | head -20
```

**5. Observable behavior**

```bash
# Detect interrupt storm: watch for rapid increase
watch -d -n 1 'head -20 /proc/interrupts'

# Check for dangerous %hi levels
mpstat -P ALL 1
```

If `%hi` exceeds 5-10%, investigate immediately. Normal systems rarely exceed 1%.

---

## 2.3.7 Real-World Failure Scenario: The "Capped" Database

**The Scenario:**
A high-performance database server hits a wall. It can't push more than 50,000 transactions/sec, even though total CPU usage is only 15%.

**The Investigation:**
1. `mpstat -P ALL 1` reveals that **CPU 0** is at 0% idle and 100% `%si` (SoftIRQ).
2. CPUs 1-31 are 98% idle.
3. `cat /proc/interrupts` shows that the network card `eth0` is delivering all interrupts solely to CPU 0.

**The Root Cause:**
**Poor IRQ Affinity**. The network card was pinned to a single core. That one core had to process every single incoming packet (TCP/IP stack, checksums, firewall rules). It became the bottleneck for the entire 32-core server.

**The Fix:**
Run `irqbalance` or manually configure `/proc/irq/N/smp_affinity` to distribute `eth0` interrupts across all cores. Throughput instantly tripled.

---

## 2.3.8 Hands-On Exercise: Triggering an Interrupt Storm

> **Warning:** Run this on a test VM only. Do not run on production.

We will generate network traffic to observe the rise in interrupts and SoftIRQ usage.

**Script Location:** `scripts/section02-03-interrupts-demo.sh`

**Steps:**
1. Open two terminals.
2. Terminal 1: Watch the interrupts.
   ```bash
   watch -n 1 "cat /proc/interrupts | grep 'loc\|eth\|ens\|virtio' | head"
   ```
   *(Note: Adjust grep filter based on your VM's interface name)*

3. Terminal 2: Run the stress script.
   ```bash
   bash scripts/section02-03-interrupts-demo.sh
   ```

**Expected Observation:**
- You should see the counters for your network interface increment rapidly.
- Run `top` or `mpstat -P ALL 1`.
- Look for a rise in `%si` (SoftIRQ).
- If the traffic is heavy enough, you might see `ksoftirqd` appear in the process list.

---

## 2.3.9 Beginner Checklist

- [ ] I can distinguish between `%hi` (Hardware IRQ) and `%si` (Software IRQ) in `top`.
- [ ] I can find which device is generating the most interrupts using `/proc/interrupts`.
- [ ] I can explain why `ksoftirqd` might consume high CPU during a file transfer or backup.
- [ ] I understand that interrupts are handled by specific CPU cores, and this can be tuned (Affinity).
- [ ] I can use `vmstat 1` to see the global rate of interrupts (`in`).
- [ ] I understand that **interrupts do NOT automatically cause context switches**—they cause mode switches.
- [ ] I can interpret `/proc/softirqs` and identify which softirq type is consuming CPU.
- [ ] I can explain what an ISR is and why it cannot sleep.
- [ ] I understand the difference between interrupt context and process context.
- [ ] I can check if a device uses MSI-X with `lspci -v`.
- [ ] I understand how NAPI reduces interrupt overhead under heavy network load.
- [ ] I can diagnose an interrupt storm using `/proc/interrupts` and `mpstat`.
