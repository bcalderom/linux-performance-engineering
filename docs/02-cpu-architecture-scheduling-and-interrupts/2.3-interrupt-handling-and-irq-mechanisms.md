# 2.3 Interrupt Handling & IRQ Mechanisms

This document explains how hardware devices get the CPU’s attention, how the kernel processes these signals, and why "system time" can spike even when your applications are idle.

**Scope:**
- The difference between Hardware Interrupts (Top Half) and Softirqs (Bottom Half).
- How to analyze `/proc/interrupts` to find hardware hotspots.
- Understanding `ksoftirqd` and `%si` CPU usage.
- Recognizing interrupt storms and single-core bottlenecks.

**Target Audience:**
Sysadmins and SREs who need to diagnose high system load (`%sys` or `%si`) or debug network throughput issues that seem to hit a "ceiling" despite available CPU capacity.

---

## 2.3.1 Key Terms

- **Hardware Interrupt (IRQ)**
  A signal from a device (like a NIC or Disk Controller) telling the CPU to stop what it's doing and handle an event immediately.
  **Observable:** `vmstat 1` (column `in`), `/proc/interrupts`.

- **Hard IRQ (Top Half)**
  The tiny, high-priority piece of code that acknowledges the hardware interrupt. It blocks all other work on that CPU core.
  **Observable:** `top` (column `%hi`).

- **SoftIRQ (Bottom Half)**
  The deferred, lower-priority work resulting from an interrupt (e.g., processing TCP packets). It runs after the Hard IRQ to avoid blocking the CPU for too long.
  **Observable:** `top` (column `%si`), process `ksoftirqd/N`.

- **Context Switching (Interrupt Context)**
  Unlike process context switches, the CPU switches into a special "Interrupt Context" to run ISRs. This is not a task switch; it's a mode switch.

- **Interrupt Affinity**
  The configuration determining which CPU cores are allowed to handle interrupts from specific devices.
  **Observable:** `/proc/irq/<IRQ>/smp_affinity`.

---

## 2.3.2 What Is an Interrupt?

If the CPU had to ask every device "Do you have data?" (Polling) millions of times a second, it would get nothing else done. Instead, devices "interrupt" the CPU.

**1. Plain-language explanation**
An interrupt is a hardware notification system. It forces the CPU to pause the current program, run a specific handler function, and then resume the program exactly where it left off.

**2. Simple analogy**
- **Polling:** You constantly check your mailbox every 10 seconds to see if mail arrived. (Wastes your time).
- **Interrupt:** The mail carrier rings your doorbell. You stop what you're doing, get the mail, and go back to work.

**3. Technical detail**
When an electrical signal arrives on an IRQ line, the CPU saves the execution state (registers) of the current task and jumps to the **Interrupt Service Routine (ISR)** defined in the kernel's Interrupt Descriptor Table (IDT).

**4. Observable behavior**
Run `vmstat 1`. The `in` column shows the number of **Interrupts per Second**.

```bash
vmstat 1
```
```text
procs ... --system-- ...
 r  b ...  in   cs ...
 1  0 ... 250  600 ...
```

**Why This Matters:**
A sudden spike in `in` (e.g., from 200 to 20,000) means hardware is screaming for attention. If the CPU spends all its time answering the doorbell, it can't do any actual work inside the house.

---

## 2.3.3 The Two-Halves Model: Hard vs. Soft IRQs

Interrupt processing is expensive. To minimize disruption, Linux splits the work.

**1. Plain-language explanation**
The "Hard IRQ" is the quick acknowledgment ("I heard you, stop ringing!"). The "SoftIRQ" is the actual work ("Okay, let me read this letter and file it").

**2. Simple analogy**
- **Hard IRQ (Receptionist):** Answers the phone immediately, takes a message, hangs up. (Fast, urgent).
- **SoftIRQ (Caseworker):** Picks up the message later and processes the request. (Slower, can wait).

**Concept Chain:**
> **Packet Arrives** → **CPU Interrupted** → **Hard IRQ (Ack Device)** → **SoftIRQ (Process TCP/IP)** → **User Application**

**3. Technical detail**
- **Hard IRQ:** Runs with local interrupts disabled. Crucial to finish fast to avoid losing other interrupts.
- **SoftIRQ:** Runs with interrupts enabled. Can be preempted. Handles heavy lifting like protocol stack processing or block I/O.

**Why This Matters:**
This split explains why you see two different CPU metrics in `top`: `%hi` (Hardware IRQ) and `%si` (Software IRQ). High `%hi` is rare and dangerous (hardware broken? driver bug?). High `%si` is common (heavy network/disk load).

---

## 2.3.4 Observing Interrupt Sources

To see *who* is ringing the doorbell, check `/proc/interrupts`.

```bash
head -n 20 /proc/interrupts
```

```text
           CPU0       CPU1       CPU2       CPU3
 24:    1234567          0          0          0   IO-APIC  24-fasteoi  eth0
 25:         10      23456          0          0   IO-APIC  25-fasteoi  ahci
```

**Interpretation:**
- **Columns:** The number of interrupts handled by each CPU core.
- **Rightmost Column:** The device name (`eth0`, `ahci`, `nvidia`).
- **Imbalance:** In the example above, `eth0` (Network) is **only** interrupting CPU0. CPU1, 2, and 3 are doing nothing for `eth0`.

> **Micro-Summary:**
> We know that devices trigger **Interrupts**, which split into **Hard** and **Soft** parts. We can identify the specific device using `/proc/interrupts`. Next, we look at what happens when there are *too many* interrupts.

---

## 2.3.5 Softirqs and ksoftirqd

When SoftIRQ work gets too heavy (e.g., 10Gbps network traffic), the kernel offloads it to special threads to protect user tasks from being totally starved.

**Observable:** `ksoftirqd/N`
These are per-CPU kernel threads (e.g., `ksoftirqd/0` for CPU 0).

| Metric | Meaning | Good/Bad |
|--------|---------|----------|
| `%hi` | Time in Hard IRQ | **>1% is Bad.** Usually indicates a hardware storm or broken driver. |
| `%si` | Time in SoftIRQ | **>30% is Concern.** High network/disk throughput. |
| `ksoftirqd` | Kernel Thread | If this process uses 100% of a core, that core is saturated with network/disk work. |

**Why This Matters:**
If `ksoftirqd/0` is at 100% usage, your application running on CPU 0 will slow down or stall, even if CPU 1-3 are idle. This is a classic **Single-Core Bottleneck**.

---

## 2.3.6 Real-World Failure Scenario: The "Capped" Database

**The Scenario:**
A high-performance database server hits a wall. It can't push more than 50,000 transactions/sec, even though total CPU usage is only 15%.

**The Investigation:**
1. `mpstat -P ALL 1` reveals that **CPU 0** is at 0% idle and 100% `%si` (SoftIRQ).
2. CPUs 1-31 are 98% idle.
3. `cat /proc/interrupts` shows that the network card `eth0` is delivering all interrupts solely to CPU 0.

**The Root Cause:**
**Poor IRQ Affinity**. The network card was pinned to a single core. That one core had to process every single incoming packet (TCP/IP stack, checksums, firewall rules). It became the bottleneck for the entire 32-core server.

**The Fix:**
Run `irqbalance` or manually configure `/proc/irq/N/smp_affinity` to distribute `eth0` interrupts across all cores. Throughput instantly tripled.

---

## 2.3.7 Hands-On Exercise: Triggering an Interrupt Storm

> **Warning:** Run this on a test VM only. Do not run on production.

We will generate network traffic to observe the rise in interrupts and SoftIRQ usage.

**Script Location:** `scripts/section02-03-interrupts-demo.sh`

**Steps:**
1. Open two terminals.
2. Terminal 1: Watch the interrupts.
   ```bash
   watch -n 1 "cat /proc/interrupts | grep 'loc\|eth\|ens\|virtio' | head"
   ```
   *(Note: Adjust grep filter based on your VM's interface name)*

3. Terminal 2: Run the stress script.
   ```bash
   bash scripts/section02-03-interrupts-demo.sh
   ```

**Expected Observation:**
- You should see the counters for your network interface increment rapidly.
- Run `top` or `mpstat -P ALL 1`.
- Look for a rise in `%si` (SoftIRQ).
- If the traffic is heavy enough, you might see `ksoftirqd` appear in the process list.

---

## 2.3.8 Beginner Checklist

- [ ] I can distinguish between `%hi` (Hardware IRQ) and `%si` (Software IRQ) in `top`.
- [ ] I can find which device is generating the most interrupts using `/proc/interrupts`.
- [ ] I can explain why `ksoftirqd` might consume high CPU during a file transfer or backup.
- [ ] I understand that interrupts are handled by specific CPU cores, and this can be tuned (Affinity).
- [ ] I can use `vmstat 1` to see the global rate of interrupts (`in`).
