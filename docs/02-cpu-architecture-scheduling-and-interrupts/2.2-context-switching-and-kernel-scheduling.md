# 2.2 Context Switching & Kernel Scheduling

This document explains how Linux decides *which* task runs *where* and *when*, and what it costs to move CPU time between tasks. It covers the mechanics of the scheduler, the runqueue, and the context switch.

**Scope:**
- The mechanics and cost of context switching.
- The role of the runqueue and the Completely Fair Scheduler (CFS).
- How to interpret Load Average correctly.
- Recognizing scheduling latency and imbalance.

**Target Audience:**
Sysadmins and SREs who notice system slowness despite low CPU usage, or who want to understand the "system" time (`%sys`) overhead in their metrics.

---

## 2.2.1 Key Terms

- **Context Switch**
  The process of saving the state of the currently running task and restoring the state of the next task.
  **Observable:** `vmstat 1` (column `cs`), `pidstat -w`.

- **Runqueue**
  A list of tasks that are ready to run (runnable) but are waiting for a CPU core to become available.
  **Observable:** `vmstat 1` (column `r`).

- **Timeslice**
  The allotted amount of time a task is allowed to run on the CPU before the scheduler considers stopping it.
  **Observable:** Indirectly via `pidstat -w` (high non-voluntary switches).

- **Preemption**
  The act of the scheduler forcibly pausing a running task to let a higher-priority or starved task run. It always leads to a context switch.
  **Observable:** `pidstat -w` (column `nvcswch/s`).

- **Voluntary Context Switch**
  When a task gives up the CPU on its own (e.g., to wait for disk I/O or a network reply). It does not lead to a context switch.
  **Observable:** `pidstat -w` (column `cswch/s`).

- **Load Average**
  A rolling average of the number of tasks in the runnable state OR uninterruptible sleep state (waiting for I/O).
  **Observable:** `uptime`, `top` (load average line).

---

## 2.2.2 What Is a Context Switch?

At any specific nanosecond, a single CPU core can only execute instructions for one task. To create the illusion of multitasking, the kernel rapidly switches between tasks.

**1. Plain-language explanation**
A context switch is the kernel's "changeover" procedure. It stops the current process, saves its spot, and loads the saved spot of the next process.

**2. Simple analogy**
Think of a single desk (the CPU) shared by multiple employees (tasks).
- Only one employee can sit at the desk at a time.
- A **context switch** is the time spent when Employee A packs up their files, leaves the chair, and Employee B sits down and unpacks their files.
- During this packing/unpacking time, **no actual work gets done**.

**3. Technical detail**
The kernel must save the CPU registers, stack pointer, and program counter of the old task, and load those of the new task. It also flushes or invalidates certain CPU caches (like the TLB), which makes the new task run slower initially until the caches warm up again.

**4. Observable behavior**
Run `vmstat 1` and watch the `cs` column.

```bash
vmstat 1
```

```text
procs ...  -system-- ...
 r  b ...  in   cs ...
 2  0 ... 500 1200 ...
```

**Why This Matters:**
Context switches are pure overhead. If your system does 100,000 switches per second, a significant chunk of CPU time is spent *managing* work rather than *doing* work.

---

## 2.2.3 The Runqueue and Scheduling Cycle

To decide who uses the desk next, the kernel maintains a queue.

**1. Plain-language explanation**
The runqueue is the line of tasks waiting for their turn on the CPU. If the line is long, tasks spend more time waiting than working.

**2. Simple analogy**
The runqueue is the **Waiting Room** at a doctor's office.
- **Runnable:** Patients in the waiting room.
- **Running:** The patient currently seeing the doctor.
- **Blocked/Sleeping:** Patients at home (not currently asking for a doctor).

**3. Technical detail**
Each CPU has its own runqueue. The scheduler (CFS) picks the task with the "fairest" claim to the CPU. When a task uses up its timeslice, it is preempted and put back into the runqueue.

**4. Observable behavior**
The `r` column in `vmstat` shows the total number of runnable tasks (waiting + running).

> **Concept Chain:**
> **Task Wakes Up** → **Enters Runqueue** → **Wait Time** → **Context Switch** → **Execution**

**Why This Matters:**
If `r` is consistently higher than your CPU count, tasks are fighting for CPU time. This competition creates **Scheduling Latency**—the delay between wanting to run and actually running.

---

## 2.2.4 The Completely Fair Scheduler (CFS)

Linux's default scheduler for normal processes is the CFS.

**1. Plain-language explanation**
CFS tries to ensure that if there are N tasks, each gets 1/N of the CPU time. It doesn't use fixed timeslices; instead, it tracks how much time each task has "earned."

**2. Simple analogy**
Imagine a group of kids sharing a single video game controller. The parent (Scheduler) ensures that whoever has played the *least* so far gets the controller next.

**3. Technical detail**
CFS tracks `vruntime` (virtual runtime). The task with the lowest `vruntime` in the runqueue is picked next. "Nice" values (priority) work by making a task's clock tick faster or slower—a low-priority task gains `vruntime` quickly, so it gets kicked off sooner.

**4. Observable behavior**
While you can't easily see `vruntime` without debug tools, you see the *result* of CFS decisions in `pidstat -w` under `nvcswch/s` (non-voluntary context switches). High non-voluntary switches mean CFS is stepping in frequently to enforce fairness.

> **Micro-Summary:**
> So far, we know that tasks wait in a **Runqueue**, the **CFS** picks the next one based on fairness, and the switch itself incurs a **Context Switch** cost. Next, we'll measure these events.

---

## 2.2.5 Measuring Switching Types

Not all context switches are bad. We distinguish them using `pidstat`.

```bash
# Report context switches (-w) every 1 second
pidstat -w 1
```

| Field | Name | Cause | Interpretation |
|-------|------|-------|----------------|
| `cswch/s` | **Voluntary** | Task needs to wait for a resource (Disk, Network, Lock). | High values usually indicate **I/O saturation** or **Lock Contention**. The CPU is not the bottleneck; the external resource is. |
| `nvcswch/s` | **Non-Voluntary** | Scheduler forcibly stopped the task (Timeslice expired, Preemption). | High values indicate **CPU saturation**. The task wanted to keep running but was kicked off. |

**Why This Matters:**
Knowing the *type* of switch tells you if you need faster disks (`cswch/s`) or more CPUs (`nvcswch/s`).

---

## 2.2.6 Load Average vs. CPU Usage

One of the most confusing metrics in Linux is the Load Average.

**1. Plain-language explanation**
Load Average is a measure of "demand" on the system, not just "busyness." It counts everyone using the CPU *plus* everyone waiting for the CPU *plus* everyone waiting for disk I/O.

**2. Simple analogy**
- **CPU Usage:** How busy the cashier is (0-100%).
- **Load Average:** How long the line is at the checkout + people currently checking out.

**3. Technical detail**
Linux Load Average counts tasks in state `R` (Runnable) and `D` (Uninterruptible Sleep / Disk Wait).

> **Local Concept Bridge:**
> To understand why a system with low CPU usage can have high load, we recall that **Load Average includes tasks waiting for Disk I/O**, not just CPU.

**4. Observable behavior**
Use `uptime`.

```text
load average: 0.50, 1.20, 4.00
```

**Interpretation Guide:**
- **Load < Number of CPUs:** System is under-utilized. No waiting.
- **Load == Number of CPUs:** System is perfectly utilized.
- **Load > Number of CPUs:** Backlog exists. Latency is increasing.

**Why This Matters:**
A high load average on a system with 0% CPU usage is a classic signature of **Disk I/O failure** or **NFS hangs**. The tasks are "stuck" in the `D` state, inflating the load number.

---

## 2.2.7 Real-World Failure Scenario: The "Chatty" Microservice

**The Scenario:**
A web server suddenly becomes unresponsive. CPU usage is only at 50% (User + Sys), but latency is huge.

**The Investigation:**
1. `uptime` shows Load Average is 20 (on an 8-core system).
2. `vmstat 1` shows `cs` (Context Switches) hitting 200,000/sec.
3. `pidstat -w 1` shows one specific service, `msg-worker`, with huge `cswch/s` (Voluntary) numbers.

**The Root Cause:**
The developer set the `msg-worker` to poll a message queue in a tight loop with a `sleep(0)` or very short timeout.
- **Chain of Events:** Thread runs → Checks Queue (Empty) → Yields CPU (Voluntary Switch) → Scheduler picks next thread → Thread runs → Checks Queue (Empty) → Yields...
- This cycle happened thousands of times per second.
- The CPU spent 50% of its time just saving/restoring registers (Kernel Time) and almost no time doing work.

**The Fix:**
Change the polling logic to use **blocking calls** (wait until data arrives) instead of rapid polling. Context switches dropped to <1000/sec, and latency vanished.

---

## 2.2.8 Scheduling Imbalance and CPU Affinity

Sometimes, one CPU core is overwhelmed while others snooze.

**Observable:** `mpstat -P ALL 1`
Look for one CPU at 0% Idle while others are at 90% Idle.

**Why This Matters:**
A single-threaded application cannot use more than 100% of one core. Adding more cores won't make it faster.

**Tuning:**
You can "pin" processes to specific cores using `taskset` to ensure critical tasks don't fight for the same core, but this is an advanced move.
**Pitfall:** Manually pinning tasks can prevent the scheduler from doing its job of balancing the load, potentially leading to *worse* performance if workloads change.

---

## 2.2.9 Hands-On Exercise: Context Switch Storm

> **Warning:** Run this on a test VM only.

This exercise uses a script to generate high voluntary and non-voluntary context switches.

**Script Location:** `scripts/section02-02-context-switching-demo.sh`

**Steps:**
1. Open two terminals.
2. In Terminal 1, start the monitor:
   ```bash
   vmstat 1
   ```
3. In Terminal 2, run the script:
   ```bash
   bash scripts/section02-02-context-switching-demo.sh
   ```

**Expected Observation:**
- As the script starts worker threads, watch the `r` column in `vmstat` exceed your CPU count.
- Watch the `cs` column skyrocket (potentially >10,000).
- Run `pidstat -w 1` to see if the switches are voluntary (I/O bound test) or non-voluntary (CPU bound test).

---

## 2.2.10 Beginner Checklist

- [ ] I can explain the difference between Voluntary and Non-Voluntary context switches.
- [ ] I can use `vmstat 1` to identify if the Runqueue (`r`) is causing latency.
- [ ] I can calculate if my Load Average indicates saturation based on my CPU count.
- [ ] I can use `pidstat -w` to find which specific process is causing excessive switching.
- [ ] I can identify "System Time" overhead caused by context switch storms.
- [ ] I understand why adding more CPUs doesn't fix a single-threaded bottleneck.
