# 2.2 Context Switching & Kernel Scheduling

This document explains how Linux decides *which* task runs *where* and *when*, and what it costs to move CPU time between tasks. It covers the mechanics of the scheduler, the runqueue, and the context switch.

**Scope:**
- The mechanics and hardware cost of context switching.
- Linux scheduling classes: CFS, Real-Time, and Deadline.
- Preemption models and tickless kernel behavior.
- Per-CPU runqueues, load balancing, and NUMA effects.
- How to interpret Load Average on multicore and virtualized systems.
- Measuring scheduling latency with `perf sched` and `cyclictest`.
- Scheduler vs IRQ affinity interactions.

**Target Audience:**
Sysadmins and SREs who notice system slowness despite low CPU usage, or who want to understand the "system" time (`%sys`) overhead in their metrics.

---

## 2.2.1 Key Terms

- **Context Switch**
  The process of saving the state of the currently running task and restoring the state of the next task.
  **Observable:** `vmstat 1` (column `cs`), `pidstat -w`.

- **Runqueue**
  A per-CPU list of tasks that are ready to run (runnable) but are waiting for CPU time.
  **Observable:** `vmstat 1` (column `r`), `/proc/schedstat`.

- **Timeslice**
  The allotted amount of time a task is allowed to run on the CPU before the scheduler considers stopping it.
  **Observable:** Indirectly via `pidstat -w` (high non-voluntary switches).

- **Preemption**
  The act of the scheduler forcibly pausing a running task to let a higher-priority or starved task run. It always leads to a context switch.
  **Observable:** `pidstat -w` (column `nvcswch/s`).

- **Voluntary Context Switch**
  When a task gives up the CPU on its own (e.g., to wait for disk I/O or a network reply).
  **Observable:** `pidstat -w` (column `cswch/s`).

- **Load Average**
  A rolling average of the number of tasks in the runnable state OR uninterruptible sleep state (waiting for I/O).
  **Observable:** `uptime`, `top` (load average line).

- **Scheduling Class**
  A category that determines how the scheduler treats a task. Linux has Deadline, Real-Time (FIFO/RR), CFS, Batch, and Idle classes.
  **Observable:** `ps -eo pid,comm,cls,pri`.

- **Scheduling Latency**
  The delay between a task becoming runnable and actually executing on a CPU.
  **Observable:** `perf sched latency`, `cyclictest`.

---

## 2.2.2 What Is a Context Switch?

At any specific nanosecond, a single CPU core can only execute instructions for one task. To create the illusion of multitasking, the kernel rapidly switches between tasks.

**1. Plain-language explanation**
A context switch is the kernel's "changeover" procedure. It stops the current process, saves its spot, and loads the saved spot of the next process.

**2. Simple analogy**
Think of a single desk (the CPU) shared by multiple employees (tasks).
- Only one employee can sit at the desk at a time.
- A **context switch** is the time spent when Employee A packs up their files, leaves the chair, and Employee B sits down and unpacks their files.
- During this packing/unpacking time, **no actual work gets done**.

**3. Technical detail**
The kernel must save the CPU registers, stack pointer, and program counter of the old task, and load those of the new task. It also flushes or invalidates certain CPU caches (like the TLB), which makes the new task run slower initially until the caches warm up again.

**4. Observable behavior**
Run `vmstat 1` and watch the `cs` column.

```bash
vmstat 1
```

```text
procs ...  -system-- ...
 r  b ...  in   cs ...
 2  0 ... 500 1200 ...
```

**Why This Matters:**
Context switches are pure overhead. If your system does 100,000 switches per second, a significant chunk of CPU time is spent *managing* work rather than *doing* work.

---

## 2.2.3 The Runqueue and Scheduling Cycle

To decide who uses the desk next, the kernel maintains a queue.

**1. Plain-language explanation**
The runqueue is the line of tasks waiting for their turn on the CPU. If the line is long, tasks spend more time waiting than working.

**2. Simple analogy**
The runqueue is the **Waiting Room** at a doctor's office.
- **Runnable:** Patients in the waiting room.
- **Running:** The patient currently seeing the doctor.
- **Blocked/Sleeping:** Patients at home (not currently asking for a doctor).

**3. Technical detail**
Each CPU has its own runqueue. The scheduler (CFS) picks the task with the "fairest" claim to the CPU. When a task uses up its timeslice, it is preempted and put back into the runqueue.

**4. Observable behavior**
The `r` column in `vmstat` shows the total number of runnable tasks (waiting + running).

> **Concept Chain:**
> **Task Wakes Up** → **Enters Runqueue** → **Wait Time** → **Context Switch** → **Execution**

**Why This Matters:**
If `r` is consistently higher than your CPU count, tasks are fighting for CPU time. This competition creates **Scheduling Latency**—the delay between wanting to run and actually running.

---

## 2.2.4 Interrupts and Scheduling: High-Level Relationship

Interrupts are covered in depth in **section 2.3**, but a brief conceptual note is necessary here:

- Interrupts cause a **mode switch**, not a task switch.
- A context switch occurs *only if*, after the interrupt, the kernel detects that:
  - a higher-priority task became runnable, or
  - a timer interrupt expired the current task's timeslice.

> **Cross-Reference:**
> For the detailed mechanics of interrupt processing (ISR, softirq flow, interrupt context, IDT, NAPI, MSI-X), see **2.3: Interrupt Handling & IRQ Mechanisms**.

---

## 2.2.5 Scheduling Classes: CFS, Real-Time, and Deadline

Linux doesn't have just one scheduler—it has a **hierarchy of scheduling classes**.

**1. Plain-language explanation**
Think of scheduling classes as priority lanes. Real-time tasks always cut in front of normal tasks. Deadline tasks cut in front of everyone.

**2. Simple analogy**
An airport has:
- **First Class boarding** (SCHED_DEADLINE): Board first, guaranteed.
- **Priority boarding** (SCHED_FIFO, SCHED_RR): Board before economy.
- **Economy boarding** (SCHED_OTHER/CFS): Board last, but fairly among themselves.

**3. Technical detail**

| Class | Policy | Behavior | Use Case |
|-------|--------|----------|----------|
| **Deadline** | `SCHED_DEADLINE` | Earliest deadline first. Guaranteed CPU time within a period. | Industrial control, audio/video processing |
| **Real-Time** | `SCHED_FIFO` | Run until voluntarily yield or blocked. No timeslice. | Latency-critical daemons |
| **Real-Time** | `SCHED_RR` | Round-robin among same-priority RT tasks. | Multiple RT tasks sharing CPU |
| **Normal** | `SCHED_OTHER` | CFS fairness-based scheduling. | 99% of user processes |
| **Batch** | `SCHED_BATCH` | CFS but optimized for throughput, not latency. | Background jobs |
| **Idle** | `SCHED_IDLE` | Runs only when nothing else wants CPU. | Lowest priority work |

**Hierarchy rule:** `DEADLINE > RT (FIFO/RR) > CFS > BATCH > IDLE`

A SCHED_FIFO task at priority 1 will **always** preempt any CFS task, regardless of nice value.

**4. Observable behavior**

```bash
# View scheduling policy and priority of processes
ps -eo pid,comm,cls,pri,ni --sort=-pri | head -20
```

```text
  PID COMMAND         CLS PRI  NI
   14 migration/0      FF 139   -
   15 watchdog/0       FF 139   -
 1234 pulseaudio       RR  50   -
 5678 nginx            TS  19   0
```

- **CLS column:** `FF` = FIFO, `RR` = Round-Robin, `TS` = Time-Sharing (CFS)
- **PRI column:** Higher = more priority (RT tasks show 139, CFS shows ~19)

> **Local Concept Bridge:**
> Now that we understand the scheduling hierarchy, let's examine CFS in detail—the scheduler that handles most of your workloads.

---

## 2.2.6 The Completely Fair Scheduler (CFS)

Linux's default scheduler for normal processes is the CFS.

**1. Plain-language explanation**
CFS tries to ensure that if there are N tasks, each gets 1/N of the CPU time. It doesn't use fixed timeslices; instead, it tracks how much time each task has "earned."

**2. Simple analogy**
Imagine a group of kids sharing a single video game controller. The parent (Scheduler) ensures that whoever has played the *least* so far gets the controller next.

**3. Technical detail**
CFS tracks `vruntime` (virtual runtime). The task with the lowest `vruntime` in the runqueue is picked next. "Nice" values (priority) work by making a task's clock tick faster or slower—a low-priority task gains `vruntime` quickly, so it gets kicked off sooner.

**4. Observable behavior**
While you can't easily see `vruntime` without debug tools, you see the *result* of CFS decisions in `pidstat -w` under `nvcswch/s` (non-voluntary context switches). High non-voluntary switches mean CFS is stepping in frequently to enforce fairness.

> **Micro-Summary:**
> So far, we know that tasks wait in a **Runqueue**, the **CFS** picks the next one based on fairness, and the switch itself incurs a **Context Switch** cost.
>
> *For how tasks become runnable due to interrupts (I/O completion, timer expiration), see section 2.3.*

---

## 2.2.7 Preemption Models and Tick Behavior

The document so far assumes a traditional "tick-based" kernel, but modern Linux kernels offer different preemption models that affect when context switches occur.

**1. Plain-language explanation**
The kernel can be configured to check for scheduling decisions at different frequencies and in different situations. A "tickless" kernel doesn't constantly interrupt running tasks; a "fully preemptible" kernel can switch tasks almost anywhere.

**2. Simple analogy**
- **Traditional tick:** A teacher checks the clock every minute and may switch students.
- **Tickless:** The teacher sets a timer for exactly when the current student's time is up—no unnecessary clock-watching.
- **Fully preemptible:** The teacher can interrupt a student mid-sentence if someone more urgent needs attention.

**3. Technical detail**

| Preemption Model | Kernel Config | Behavior | Use Case |
|------------------|---------------|----------|----------|
| **None** | `PREEMPT_NONE` | Only preempt at explicit schedule points. | Throughput-optimized servers |
| **Voluntary** | `PREEMPT_VOLUNTARY` | Preempt at explicit points + some kernel locations. | General-purpose servers |
| **Preemptible** | `PREEMPT` | Preempt almost anywhere except critical sections. | Desktop, low-latency workloads |
| **Full RT** | `PREEMPT_RT` | Nearly all kernel code is preemptible. | Real-time systems, audio production |

**Tick behavior:**

| Mode | Kernel Config | Behavior |
|------|---------------|----------|
| **Periodic** | `HZ=250/1000` | Timer fires 250-1000 times/sec on all CPUs. |
| **Idle tickless** | `NO_HZ_IDLE` | Stop ticks on idle CPUs to save power. |
| **Full tickless** | `NO_HZ_FULL` | Stop ticks even on busy CPUs (one task running). |

**4. Observable behavior**

```bash
# Check your kernel's preemption model
cat /boot/config-$(uname -r) | grep PREEMPT

# Check tick configuration
cat /boot/config-$(uname -r) | grep NO_HZ

# See current timer frequency
grep CONFIG_HZ /boot/config-$(uname -r)
```

**Why This Matters:**
- **Latency-sensitive apps:** A `PREEMPT` or `PREEMPT_RT` kernel reduces scheduling latency.
- **Throughput workloads:** `PREEMPT_NONE` reduces context switch overhead.
- **Tickless kernels:** Affect how you interpret `%sys` and timer interrupt counts—fewer ticks mean fewer interrupts but potentially longer scheduling delays.

---

## 2.2.8 Measuring Switching Types: Voluntary vs Non-Voluntary

Not all context switches are bad. We distinguish them using `pidstat`.

```bash
# Report context switches (-w) every 1 second
pidstat -w 1
```

| Field | Name | Cause | Interpretation |
|-------|------|-------|----------------|
| `cswch/s` | **Voluntary** | Task needs to wait for a resource (Disk, Network, Lock). | High values usually indicate **I/O saturation** or **Lock Contention**. The CPU is not the bottleneck; the external resource is. |
| `nvcswch/s` | **Non-Voluntary** | Scheduler forcibly stopped the task (Timeslice expired, Preemption). | High values indicate **CPU saturation**. The task wanted to keep running but was kicked off. |

### 2.2.8.1 Detailed Causes of Context Switches

**Voluntary Context Switches** occur when a task explicitly yields the CPU:

| Trigger | Syscall/Mechanism | Example Scenario |
|---------|-------------------|------------------|
| Blocking I/O | `read()`, `write()` on disk/network | Reading a file, waiting for network data |
| Sleep/Delay | `sleep()`, `nanosleep()`, `usleep()` | Intentional delays in application logic |
| Mutex contention | `pthread_mutex_lock()` | Thread waiting for a lock held by another |
| Futex wait | `futex(FUTEX_WAIT)` | Userspace synchronization primitives |
| Semaphore wait | `sem_wait()` | Producer-consumer coordination |
| Condition variable | `pthread_cond_wait()` | Waiting for a signal from another thread |
| Poll/Select | `poll()`, `select()`, `epoll_wait()` | Event-driven I/O with no ready events |
| Message queue | `mq_receive()` (blocking) | IPC waiting for messages |

**Non-Voluntary Context Switches** occur when the scheduler forcibly preempts a task:

| Trigger | Mechanism | Example Scenario |
|---------|-----------|------------------|
| Timeslice expiration | CFS vruntime exceeded | CPU-bound task ran "too long" |
| Higher-priority wakeup | RT task becomes runnable | Real-time task preempts normal task |
| Preemption point | Kernel `cond_resched()` | Long-running kernel code yields |
| Load balancing | Scheduler migration | Task moved to balance CPU load |
| Interrupt wakeup | I/O completion wakes higher-priority task | Disk read completes for waiting task |

**Why This Matters:**
Knowing the *type* of switch tells you if you need faster disks (`cswch/s`) or more CPUs (`nvcswch/s`).

---

## 2.2.9 Per-CPU Runqueues, Load Balancing, and NUMA

The runqueue isn't a single global queue—each CPU has its own.

**1. Plain-language explanation**
Each CPU maintains its own list of tasks to run. The scheduler periodically balances work between CPUs to prevent one from being overloaded while others idle.

**2. Simple analogy**
A supermarket with multiple checkout lanes (CPUs). Each lane has its own queue (runqueue). A manager (load balancer) periodically moves customers between lanes to keep wait times even.

**3. Technical detail**

**Per-CPU architecture:**
- Each CPU has a `struct rq` (runqueue) containing ready tasks.
- Tasks have **CPU affinity**—they prefer to stay on the same CPU for cache warmth.
- The scheduler uses **push** and **pull** migration:
  - **Push:** Overloaded CPU sends tasks to idle CPUs.
  - **Pull:** Idle CPU steals tasks from busy CPUs.

**NUMA considerations:**
- On NUMA systems, memory is "closer" to some CPUs than others.
- Migrating a task to a different NUMA node means its memory accesses become slower (remote memory).
- The scheduler tries to keep tasks on their "home" NUMA node.

| Migration Type | Cost | When It Happens |
|----------------|------|----------------|
| Same-core (hyperthread) | Lowest | Sibling thread idle |
| Same-socket, different core | Low | Core idle, cache shared at L3 |
| Different socket (NUMA) | High | Severe imbalance, no local CPUs idle |

**4. Observable behavior**

```bash
# View per-CPU runqueue depth
cat /proc/schedstat | awk '{print "CPU"NR-1": runqueue="$2}'

# Watch task migrations
perf stat -e migrations -a sleep 5

# Check NUMA topology
numactl --hardware

# See which NUMA node a process's memory is on
numastat -p <pid>
```

**Why This Matters:**
- **High migration counts** indicate tasks bouncing between CPUs, causing cache misses.
- **NUMA imbalance** can cause 2-3x memory latency increases.
- **Pinning tasks** (`taskset`, `numactl`) can help but may cause other imbalances.

> **Micro-Summary:**
> Tasks live in per-CPU runqueues. The scheduler balances load but tries to minimize migrations due to cache and NUMA costs.

---

## 2.2.10 The Real Cost of a Context Switch

Context switches aren't free. Understanding their hardware cost helps explain why minimizing them matters.

**1. Plain-language explanation**
A context switch forces the CPU to throw away work it has already done (cached data, predicted branches) and start fresh with the new task.

**2. Simple analogy**
Switching tasks is like a chef changing dishes mid-preparation:
- Put away all ingredients for Dish A.
- Clean the workspace.
- Get out all ingredients for Dish B.
- Remember where you were in the recipe.

The more complex the dish (task), the more time lost in switching.

**3. Technical detail**

**Direct costs (always incurred):**

| Component | What Happens | Typical Cost |
|-----------|--------------|-------------|
| Register save/restore | All CPU registers saved to memory, new ones loaded | ~100-200 cycles |
| Stack pointer switch | Kernel switches to new task's kernel stack | ~10 cycles |
| TLB flush | Translation Lookaside Buffer invalidated (virtual→physical mappings) | ~100-1000 cycles |
| Kernel overhead | Scheduler logic, accounting | ~500-2000 cycles |

**Indirect costs (cache effects):**

| Component | What Happens | Typical Cost |
|-----------|--------------|-------------|
| L1 cache cold | New task's data not in L1 (32KB) | ~4 cycles per miss |
| L2 cache cold | New task's data not in L2 (256KB) | ~12 cycles per miss |
| L3 cache cold | New task's data not in L3 (shared) | ~40 cycles per miss |
| Instruction cache cold | New task's code not cached | Similar to data cache |
| Branch predictor pollution | CPU's branch predictions are wrong | ~10-20 cycles per mispredict |

**Measured costs on modern hardware:**

| Switch Type | Typical Latency | Notes |
|-------------|-----------------|-------|
| User → User (same process, threads) | 1-2 µs | Shared address space, minimal TLB impact |
| User → User (different processes) | 3-5 µs | Full TLB flush, address space switch |
| User → Kernel (syscall) | 0.5-1 µs | Mode switch only, no task switch |
| Cross-CPU migration | 5-15 µs | Cache completely cold on new CPU |
| Cross-NUMA migration | 10-30 µs | Memory latency increases significantly |

**4. Observable behavior**

```bash
# Measure context switch overhead with perf
perf bench sched messaging

# Measure with hackbench
hackbench -g 10 -l 1000

# See cache misses correlated with switches
perf stat -e context-switches,cache-misses,LLC-load-misses -a sleep 10
```

**Why This Matters:**
At 100,000 context switches/second with 5µs each, you lose 500ms of CPU time per second (50% overhead) just to switching—before counting cache warm-up costs.

---

## 2.2.11 Measuring Scheduling Latency

Scheduling latency is the delay between a task becoming runnable and actually running.

**1. Plain-language explanation**
When a task wakes up (e.g., I/O completes), it doesn't run immediately. It waits in the runqueue until the scheduler picks it. This wait time is scheduling latency.

**2. Simple analogy**
You're at a restaurant. You finish deciding (become runnable), but the waiter is busy with other tables. The time until the waiter takes your order is scheduling latency.

**3. Technical detail**

Scheduling latency has multiple components:

| Component | Description | Typical Range |
|-----------|-------------|---------------|
| **Wakeup latency** | Time from event (IRQ) to task marked runnable | 1-10 µs |
| **Runqueue wait** | Time waiting for CPU in runqueue | 0-100+ ms |
| **Preemption delay** | Time for current task to yield | 0-4 ms (CFS), 0 (RT) |

**4. Observable behavior**

```bash
# Use perf sched to analyze scheduling latency
perf sched record -- sleep 10
perf sched latency

# Sample output:
# Task                  |   Runtime ms  | Switches | Avg delay ms | Max delay ms |
# nginx                 |    234.567    |   1234   |    0.045     |    12.345    |
# postgres              |    567.890    |    567   |    0.123     |    45.678    |
```

```bash
# Detailed timeline view
perf sched timehist

# Sample output:
#  time      cpu  task name         wait time  sch delay   run time
# 1000.001   [0]  nginx:1234         0.000 ms   0.045 ms    2.345 ms
# 1000.003   [0]  postgres:5678      1.234 ms   0.067 ms    5.678 ms
```

```bash
# For real-time latency testing (requires rt-tests package)
cyclictest -p 80 -t 4 -n -l 10000

# Sample output:
# T: 0 ( 1234) P:80 I:1000 C:  10000 Min:      1 Act:    5 Avg:    3 Max:   45
# T: 1 ( 1235) P:80 I:1500 C:  10000 Min:      1 Act:    4 Avg:    3 Max:   52
```

**Interpreting results:**
- **Avg delay < 1ms:** Normal for CFS workloads.
- **Max delay > 10ms:** Investigate runqueue depth or preemption issues.
- **Max delay > 100ms:** Serious scheduling problem—check for RT tasks, kernel bugs, or CPU starvation.

**Why This Matters:**
High scheduling latency directly impacts application response time. A web server waiting 50ms in the runqueue adds 50ms to every request.

---

## 2.2.12 Load Average on Multicore and Virtualized Systems

The basic Load Average interpretation needs adjustment for modern systems.

**1. Plain-language explanation**
Load Average is a measure of "demand" on the system, not just "busyness." It counts everyone using the CPU *plus* everyone waiting for the CPU *plus* everyone waiting for disk I/O.

**2. Simple analogy**
- **CPU Usage:** How busy the cashier is (0-100%).
- **Load Average:** How long the line is at the checkout + people currently checking out.

**3. Technical detail**
Linux Load Average counts tasks in state `R` (Runnable) and `D` (Uninterruptible Sleep / Disk Wait).

> **Local Concept Bridge:**
> To understand why a system with low CPU usage can have high load, we recall that **Load Average includes tasks waiting for Disk I/O**, not just CPU.

**4. Observable behavior**
Use `uptime`.

```text
load average: 0.50, 1.20, 4.00
```

**Interpretation Guide:**
- **Load < Number of CPUs:** System is under-utilized. No waiting.
- **Load == Number of CPUs:** System is perfectly utilized.
- **Load > Number of CPUs:** Backlog exists. Latency is increasing.

### 2.2.12.1 Load Average on Multicore Systems

A common mistake is panicking at "high" load numbers without considering CPU count.

| CPUs | Load 1.0 | Load 4.0 | Load 16.0 |
|------|----------|----------|----------|
| 1 | Saturated | 4x overloaded | Severe |
| 4 | 25% utilized | Saturated | 4x overloaded |
| 16 | 6% utilized | 25% utilized | Saturated |
| 64 | Idle | Idle | 25% utilized |

**Rule of thumb:** Load Average / CPU Count = Utilization ratio.
- Ratio < 0.7: Comfortable headroom.
- Ratio 0.7-1.0: Approaching saturation.
- Ratio > 1.0: Backlog exists.

### 2.2.12.2 D-State Tasks Inflating Load

Tasks in `D` (Uninterruptible Sleep) state count toward load average but don't use CPU.

```bash
# Find D-state tasks
ps aux | awk '$8 ~ /D/ {print $0}'

# Or with process state
ps -eo pid,stat,comm | grep "^.*D"
```

**Common causes of D-state:**
- NFS server unreachable
- Disk I/O stall (failing drive, full queue)
- Kernel driver bug
- iSCSI/SAN timeout

### 2.2.12.3 Load Average in Virtualized Environments

In VMs (EC2, GCP, Azure), load average interpretation changes:

| Scenario | What You See | Reality |
|----------|--------------|--------|
| CPU steal (`%st`) high | Load looks normal | Hypervisor taking your CPU time |
| Overcommitted host | Load spikes unpredictably | Other VMs competing |
| Burstable instances (T2/T3) | Sudden performance drop | CPU credits exhausted |

```bash
# Check for CPU steal time
mpstat 1 | grep -E "CPU|all"

# Look for %steal column > 5%
```

**Why This Matters:**
A VM with load 2.0 on 2 vCPUs and 20% steal time is actually CPU-starved—the hypervisor is taking your cycles.

---

## 2.2.13 Scheduler vs IRQ Affinity Pitfalls

CPU affinity for tasks and IRQs can interact in unexpected ways.

**1. Plain-language explanation**
You can pin tasks to specific CPUs (`taskset`) and pin interrupt handlers to specific CPUs (`irqbalance`, `/proc/irq/*/smp_affinity`). When both are pinned to the same CPU, they compete.

**2. Simple analogy**
You assign one employee (CPU) to handle both phone calls (IRQs) and customer orders (tasks). During a phone call flood, customers wait.

**3. Technical detail**

**Co-location benefits:**
- Task and its IRQs on same CPU = better cache locality.
- Network task on same CPU as NIC IRQ = data already in cache.

**Co-location risks:**
- High IRQ rate starves the task.
- Softirq processing delays task execution.
- `ksoftirqd` may consume significant CPU time.

| Configuration | Benefit | Risk |
|---------------|---------|------|
| Task + IRQ on same CPU | Cache locality | IRQ starvation |
| Task + IRQ on different CPUs | Isolation | Cache misses, cross-CPU wakeup |
| `irqbalance` automatic | Adaptive | May conflict with manual pinning |

**4. Observable behavior**

```bash
# See current IRQ affinity
cat /proc/irq/*/smp_affinity_list

# See which CPUs handle which IRQs
watch -n1 'cat /proc/interrupts | head -20'

# Check if irqbalance is running
systemctl status irqbalance

# See task CPU affinity
taskset -p <pid>
```

**Pitfall example:**
```bash
# You pin nginx to CPU 0
taskset -c 0 nginx

# But irqbalance puts NIC IRQs on CPU 0 too
# During traffic spike, nginx starves while softirqs run
```

**Best practice:**
- For latency-sensitive tasks, isolate CPUs from IRQs using `isolcpus` kernel parameter.
- Or dedicate specific CPUs to IRQ handling and others to application work.

---

## 2.2.14 Real-World Failure Scenario: The "Chatty" Microservice

**The Scenario:**
A web server suddenly becomes unresponsive. CPU usage is only at 50% (User + Sys), but latency is huge.

**The Investigation:**
1. `uptime` shows Load Average is 20 (on an 8-core system).
2. `vmstat 1` shows `cs` (Context Switches) hitting 200,000/sec.
3. `pidstat -w 1` shows one specific service, `msg-worker`, with huge `cswch/s` (Voluntary) numbers.

**The Root Cause:**
The developer set the `msg-worker` to poll a message queue in a tight loop with a `sleep(0)` or very short timeout.
- **Chain of Events:** Thread runs → Checks Queue (Empty) → Yields CPU (Voluntary Switch) → Scheduler picks next thread → Thread runs → Checks Queue (Empty) → Yields...
- This cycle happened thousands of times per second.
- The CPU spent 50% of its time just saving/restoring registers (Kernel Time) and almost no time doing work.

**The Fix:**
Change the polling logic to use **blocking calls** (wait until data arrives) instead of rapid polling. Context switches dropped to <1000/sec, and latency vanished.

---

## 2.2.15 Real-World Failure Scenario: CPU-Bound Starvation

**The Scenario:**
A batch processing server runs multiple data transformation jobs. Users report that some jobs take 10x longer than expected, while others finish quickly.

**The Investigation:**
1. `top` shows CPU at 100% across all cores.
2. `pidstat -w 1` shows several processes with extremely high `nvcswch/s` (non-voluntary switches).
3. `ps -eo pid,ni,comm --sort=-ni` reveals one process running at nice -20 (highest priority).

**The Root Cause:**
An administrator ran a "critical" backup job with `nice -n -20`, giving it maximum CPU priority.
- **Chain of Events:** Backup job runs → Other jobs get preempted → CFS gives backup most CPU time → Other jobs starve.
- The backup job consumed 80% of available CPU time.
- Other jobs were constantly preempted (high non-voluntary switches).

**The Fix:**
1. Run batch jobs at equal or lower priority: `nice -n 10 backup_job`.
2. Use cgroups to limit CPU shares for non-critical jobs.
3. Consider `SCHED_BATCH` for throughput-oriented jobs that don't need low latency.

```bash
# Limit a process to 50% of one CPU using cgroups v2
echo "50000 100000" > /sys/fs/cgroup/batch_jobs/cpu.max
```

---

## 2.2.16 Real-World Failure Scenario: I/O-Bound Voluntary Switch Storm

**The Scenario:**
A database server shows load average of 50 on an 8-core system, but CPU usage is only 15%. Response times are terrible.

**The Investigation:**
1. `uptime` shows load average 50 (on 8 CPUs).
2. `top` shows only 15% CPU usage, but 70% in I/O wait (`%wa`).
3. `pidstat -w 1` shows database processes with massive `cswch/s` (voluntary switches).
4. `iostat -x 1` shows disk at 100% utilization with high await times.

**The Root Cause:**
The database was performing full table scans on a slow HDD.
- **Chain of Events:** Query runs → Needs data from disk → Blocks (voluntary switch) → Waits for slow I/O → Wakes up → Needs more data → Blocks again...
- Each query generated hundreds of voluntary switches.
- Tasks piled up in `D` state, inflating load average.
- CPU sat idle waiting for disk.

**The Fix:**
1. Add missing database indexes to reduce full table scans.
2. Increase database buffer pool to cache more data in RAM.
3. Migrate to SSD storage for faster I/O.
4. Tune `vm.dirty_ratio` and `vm.dirty_background_ratio` for better write batching.

```bash
# Check current I/O scheduler
cat /sys/block/sda/queue/scheduler

# For SSDs, use 'none' or 'mq-deadline'
echo "mq-deadline" > /sys/block/sda/queue/scheduler
```

> **Micro-Summary:**
> High load + low CPU + high voluntary switches = I/O bottleneck. The solution is faster storage or less I/O, not more CPUs.

---

## 2.2.17 Hands-On Exercise: Context Switch Storm

> **Warning:** Run this on a test VM only.

This exercise uses a script to generate high voluntary and non-voluntary context switches.

**Script Location:** `scripts/section02-02-context-switching-demo.sh`

**Steps:**
1. Open two terminals.
2. In Terminal 1, start the monitor:
   ```bash
   vmstat 1
   ```
3. In Terminal 2, run the script:
   ```bash
   bash scripts/section02-02-context-switching-demo.sh
   ```

**Expected Observation:**
- As the script starts worker threads, watch the `r` column in `vmstat` exceed your CPU count.
- Watch the `cs` column skyrocket (potentially >10,000).
- Run `pidstat -w 1` to see if the switches are voluntary (I/O bound test) or non-voluntary (CPU bound test).

---

## 2.2.18 Beginner Checklist

- [ ] I can explain the difference between Voluntary and Non-Voluntary context switches.
- [ ] I can use `vmstat 1` to identify if the Runqueue (`r`) is causing latency.
- [ ] I can calculate if my Load Average indicates saturation based on my CPU count.
- [ ] I can use `pidstat -w` to find which specific process is causing excessive switching.
- [ ] I can identify "System Time" overhead caused by context switch storms.
- [ ] I understand why adding more CPUs doesn't fix a single-threaded bottleneck.
- [ ] I understand the high-level relationship between interrupts and scheduling (full interrupt mechanics are in section 2.3).
- [ ] I can identify the scheduling class of a process using `ps`.
- [ ] I can check my kernel's preemption model and tick configuration.
- [ ] I can use `perf sched latency` to measure scheduling delays.
- [ ] I can diagnose D-state tasks inflating load average.
- [ ] I can identify CPU steal time in virtualized environments.
- [ ] I understand the trade-offs of CPU affinity and IRQ pinning.
