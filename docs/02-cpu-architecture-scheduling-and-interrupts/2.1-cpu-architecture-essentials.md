# 2.1 CPU Architecture Essentials

This document introduces the physical and logical structure of modern CPUs as seen by Linux. It focuses on how cores, hardware threads, caches, and frequency scaling shape performance, and how you can *observe* these details using standard tools.

The goal is not to turn you into a CPU designer, but to give you enough intuition to reason about why a workload is fast or slow, noisy or stable, and what “CPU-bound” really means in practice.

---

## 2.1.1 Key Terms

- **CPU core**  
  An independent execution unit that can run instructions. A modern server has multiple cores per socket.

- **Hardware thread (logical CPU)**  
  A schedulable execution slot exposed to the OS, often created by technologies like Hyper-Threading (two hardware threads per core).

- **Socket**  
  A physical CPU package on the motherboard. A dual-socket system has two CPU packages, each with its own cores and caches.

- **Cache hierarchy (L1/L2/L3)**  
  Small, fast memories on the CPU that store recently used data and instructions. Organized in levels: L1 is smallest and fastest, L3 is largest and slowest (but still much faster than RAM).

- **NUMA (Non-Uniform Memory Access)**  
  A memory architecture where access latency and bandwidth depend on which socket owns the memory.

- **Clock frequency**  
  How fast a core executes cycles per second (GHz). With turbo/boost, frequency changes dynamically based on load and thermal limits.

- **IPC (Instructions Per Cycle)**  
  How many instructions a core retires each clock cycle on average. Higher IPC generally means better per-core performance at the same frequency.

- **CPU-bound workload**  
  A workload whose throughput or latency is primarily limited by CPU execution resources rather than I/O or the network.

All of these terms will be used and explained in the sections that follow.

---

## 2.1.2 Why CPU Architecture Matters for Performance

From a distance, a CPU looks like a single number in `top`: `%Cpu(s): 85.3 us, 3.1 sy, ...`. Under the hood, that number hides:

- How many *cores* you have.
- How many *hardware threads* (logical CPUs) per core.
- How effective your *caches* are.
- Whether your workload is *CPU-bound* or blocked on something else.

An analogy:

- Imagine a restaurant kitchen.
- **Cores** are chefs.
- **Hardware threads** are each chef’s two hands; they can work on two tasks, but they still share one brain and one body.
- **Caches** are ingredients on the counter; RAM is the pantry down the hallway.
- If the chefs constantly run back to the pantry, service slows down – even if everyone is working hard.

Linux exposes this structure via `/proc/cpuinfo`, `/sys/devices/system/cpu/`, and tools like `lscpu`, `nproc`, and `htop`. Understanding this structure helps you answer questions such as:

- Do I really have free CPU capacity, or am I saturating certain cores only?
- Is Hyper-Threading helping this workload, or just adding noise and contention?
- Is performance limited by raw CPU cycles, memory latency, or cache behavior?

---

## 2.1.3 Seeing Cores, Sockets, and Threads with lscpu

The `lscpu` command summarizes the CPU topology from `/proc/cpuinfo` and sysfs.

```bash
lscpu
```

Sample (abridged) output:

```text
Architecture:        x86_64
CPU(s):              8
On-line CPU(s) list: 0-7
Thread(s) per core:  2
Core(s) per socket:  4
Socket(s):           1
NUMA node(s):        1
Model name:          Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz
CPU MHz:             2399.996
NUMA node0 CPU(s):   0-7
```

Key fields and how to interpret them:

- **CPU(s)**  
  Total number of hardware threads (logical CPUs) available to Linux.

- **Thread(s) per core**  
  How many hardware threads share one physical core. Value >1 indicates Hyper-Threading or similar.

- **Core(s) per socket** and **Socket(s)**  
  Multiply these together and then by threads-per-core to cross-check against `CPU(s)`.

- **NUMA node(s)** and `NUMA nodeX CPU(s)`  
  Show which logical CPUs belong to each memory node.

For performance investigation, always translate “CPU is 90% busy” into “how many cores and threads are doing work?” A load of 7 on an 8-thread system means something entirely different from 7 on a 64-thread system.

---

## 2.1.4 CPU Topology in Linux: A Visual Overview

The following Mermaid diagram shows a simplified single-socket system with 4 cores and 2 hardware threads per core:

```mermaid
graph TD
    A[Socket 0] --> B[Core 0]
    A --> C[Core 1]
    A --> D[Core 2]
    A --> E[Core 3]

    B --> B0[CPU 0]
    B --> B1[CPU 1]
    C --> C0[CPU 2]
    C --> C1[CPU 3]
    D --> D0[CPU 4]
    D --> D1[CPU 5]
    E --> E0[CPU 6]
    E --> E1[CPU 7]

    A --- L3[L3 Cache (shared)]
    B --- L2B[L2 Cache]
    C --- L2C[L2 Cache]
    D --- L2D[L2 Cache]
    E --- L2E[L2 Cache]
```

In Linux tools:

- `CPU 0..7` correspond to logical CPUs in `lscpu` and `htop`.
- Per-core shared caches (L2) and socket-level shared cache (L3) influence whether threads interfere with each other.

When you pin multiple hot threads to the same core (same two logical CPUs), they will compete for the same execution units and caches. That can reduce throughput and increase latency even when *system-wide* CPU utilization still looks moderate.

---

## 2.1.5 Cache Hierarchy and Why It Matters

Modern CPUs rely heavily on caches to hide main memory latency.

- **L1 cache**  
  Extremely small (tens of KB per core) and very fast.

- **L2 cache**  
  Larger (hundreds of KB per core), slightly slower but still much faster than RAM.

- **L3 cache (last-level cache)**  
  Shared across cores inside a socket. Size is in MB.

Analogy:

- L1: ingredients in a chef’s hands.
- L2: nearby countertop.
- L3: common prep table in the kitchen.
- RAM: pantry room.

If the working set (the data you touch frequently) fits into cache, performance can be an order of magnitude better than if every access goes to RAM. When caches are constantly evicted, you see more CPU cycles spent waiting on memory. At the Linux CLI, that often looks like:

- High CPU utilization with relatively low system time.
- Performance sensitive to data size even when algorithmic complexity is unchanged.

You cannot see cache hits directly with basic tools, but you *can* use performance counters (e.g., `perf stat`) to get cache miss statistics:

```bash
sudo perf stat -e cycles,instructions,LLC-loads,LLC-load-misses -- \
    bash -c 'for i in {1..5}; do sha256sum /bin/bash >/dev/null; done'
```

Look for:

- **instructions per cycle** (IPC = instructions / cycles).  
  Lower IPC may signal stalls (including cache misses) or heavy branch mispredictions.
- **LLC-load-misses** relative to **LLC-loads**.  
  A high miss ratio can indicate poor cache locality.

For many sysadmins, the key takeaway is: data placement and access patterns can change CPU time dramatically, even for the same amount of *work* at the application level.

---

## 2.1.6 NUMA Basics and Cross-Node Penalties

On a NUMA system, memory attached to one socket is “closer” (lower latency, higher bandwidth) to that socket’s cores than to the other socket’s cores.

Linux exposes NUMA layout with:

```bash
numactl --hardware
```

Example (simplified):

```text
available: 2 nodes (0-1)
node 0 cpus: 0 1 2 3 4 5 6 7
node 1 cpus: 8 9 10 11 12 13 14 15
node 0 size: 64000 MB
node 1 size: 64000 MB
```

Conceptually:

- Accessing memory local to the same NUMA node is like walking to the pantry inside your own kitchen.
- Accessing remote memory from another node is like walking to the neighbor’s kitchen.

In practice:

- Remote memory access can add tens of nanoseconds of latency per access.
- A NUMA-ignorant workload might jump around sockets, suffering additional latency and cross-socket traffic.

For many small systems (single socket), NUMA is effectively invisible. On multi-socket servers, you should always keep it in mind when you see unexpected latency under load or uneven CPU utilization (e.g., one socket hot, the other idle).

---

## 2.1.7 Frequency Scaling, Turbo, and Thermal Limits

The **clock frequency** of a core is not fixed:

- At idle, frequency is reduced to save power.
- Under bursty workloads, frequency can increase (turbo/boost) for short periods.
- Under sustained heavy loads with thermal constraints, frequency can drop again.

You can see current frequencies with:

```bash
watch -n 1 "grep 'cpu MHz' /proc/cpuinfo | head"
```

Or more compactly with `lscpu` and `cpupower` (if installed):

```bash
cpupower frequency-info
```

Performance implications:

- Short interactive tasks may enjoy high turbo frequencies and appear fast.
- Long-running CPU-bound tasks may settle at lower frequencies once thermal headroom disappears.

When benchmarking or troubleshooting “it was fast yesterday, slow today,” always consider ambient temperature, thermal throttling, and power-management settings.

---

## 2.1.8 Identifying CPU-Bound Workloads

A workload is **CPU-bound** when increasing CPU capacity (more cores or higher frequency) significantly improves throughput or latency, and other subsystems (disk, network) are not saturated.

You can get an initial sense with `top` or `htop`:

```bash
top
```

Look for:

- High `%us` (user) and `%sy` (system) with low `%wa` (I/O wait).
- One or more processes near 100% CPU on a single logical CPU.
- Load average roughly proportional to the number of busy CPUs.

For a more detailed view, use `pidstat` (from `sysstat`) or `perf top` to see which functions or instructions are consuming CPU.

Once you know a workload is CPU-bound, Section 02 as a whole helps you reason about *why*: poor cache usage, contention between hardware threads, NUMA penalties, or simply not enough cores.

---

## 2.1.9 Hands-On Exercise: Saturating a Single Core

> **Warning:** Run this only on a non-production system. It will temporarily fully utilize one logical CPU.

Use this small script to create a CPU-bound workload and observe it from Linux tools.

Associated script: `scripts/section02-01-cpu-architecture-demo.sh`.

Run it as:

```bash
bash scripts/section02-01-cpu-architecture-demo.sh
```

While it runs, in another terminal:

- Run `top` and watch a single process stay near 100% on one CPU.
- Run `lscpu` to see how many logical CPUs you have.
- Compare the single hot CPU to total capacity.

Questions to ask yourself:

- On a system with 8 logical CPUs, does 100% on one CPU mean the *system* is fully loaded?
- What happens if you increase the number of worker processes in the script?

---

## 2.1.10 Beginner Checklist

- [ ] I can run `lscpu` and explain the difference between sockets, cores, and hardware threads.
- [ ] I can describe, in plain language, what CPU caches are and why they matter for performance.
- [ ] I can explain what NUMA is and how to see NUMA layout with `numactl --hardware`.
- [ ] I can use `top` or `htop` to identify CPU-bound workloads on my system.
- [ ] I can run `section02-01-cpu-architecture-demo.sh` and interpret what I see in `top` and `lscpu`.
- [ ] I can explain why “CPU is 90%” is incomplete without knowing how many cores and threads are available.
